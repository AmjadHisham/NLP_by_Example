{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Filtering Using Naive Bays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mails are one of the major ways of official communications and out mail servers are having algorithm to tag any incoming mail as spam and move the same to a certain folder named \"SPAM Folder\"\n",
    "Below we are going to see an experiment on the similar case and taking some sample mail we shall build a supervised classification model to detect the given mail is spam or not.\n",
    "\n",
    "#### We are using use Enron dataset for this experimentation: \n",
    "##### Dataset Link: http://www2.aueb.gr/users/ion/data/enron-spam/\n",
    "\n",
    "It contains a preclassified text files containing mails. \n",
    "### Please feel free to download and experiment on the same. Please download and save the data by creating a folder named   \"dataset\"\n",
    "\n",
    "#### Let's import few necessery packages before we start our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkpathak\\AppData\\Local\\Continuum\\anaconda2\\envs\\py34t\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generally you may receive multiple text files on Enron dataset which needs to be merged and leveled for further analysis. We are going to create a data frame file containing two columns. Data frame is just like our excel file with different columns and rows. Column represents the type of field and rows represents the data element \n",
    "        Text -> This column would contain the text body of each mail\n",
    "        Class -> This column contains the lebel of whether the mail is 'spam' or 'ham'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step:  \n",
    "Creating a data-frame with two columns Text= the extracted texts from each mail and Class represents whether it is \"Spam\" and \"Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_data = pd.DataFrame(columns = (\"Text\", \"Class\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cla in glob.glob(\"dataset/*\"): # Here the folder name is data set in which there are two sub-folder \"Spam\" & \"Ham\"\n",
    "    clas = cla.split(os.sep)[1]    # We are splitting the folder names as class using OS-Seperator and taking the 2nd item in the list\n",
    "    for file in glob.glob(cla + \"/*.txt\"): # Here we are deep diving in each of the folder and reading the text files one by one\n",
    "        text = open(file, \"r\", encoding = \"ISO-8859-1\").read() # Reading the file , for Windows generally we need to mention the encoding \n",
    "        text = \" \".join(text.split(\"\\n\")) # Splitting the text files and rejoining into a single text\n",
    "        spam_data = spam_data.append(pd.Series([text, clas], index = [\"Text\", \"Class\"]), ignore_index = True) # continious append to the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to get a feel on how the data frame looks like? Let's see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: christmas tree farm pictures</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: vastar resources , inc . gary , produ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: calpine daily gas nomination - calpin...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: re : issue fyi - see note below - alr...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: meter 7268 nov allocation fyi . - - -...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Class\n",
       "0             Subject: christmas tree farm pictures    ham\n",
       "1  Subject: vastar resources , inc . gary , produ...   ham\n",
       "2  Subject: calpine daily gas nomination - calpin...   ham\n",
       "3  Subject: re : issue fyi - see note below - alr...   ham\n",
       "4  Subject: meter 7268 nov allocation fyi . - - -...   ham"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the class label into binary outcome variable for convenience.\n",
    "1- in case the mail is a spam mail\n",
    "\n",
    "0- in case the mail is not a spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "spam_data['Class'] = spam_data.Class.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As next step we need to clean the data and remove stopwords:\n",
    "    This involves cleaning of alpha neumerical characters/ special characters \"@\" , \"-\", \"1\" etc\n",
    "    Any stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_complete = spam_data['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: mcmullen gas for 11 / 99 jackie , since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of flow ) : at what meter is the mcmullen gas being diverted to ? at what meter is hpl buying the residue gas ? ( this is the gas from teco , vastar , vintage , tejones , and swift ) i still see active deals at meter 3405 in path manager for teco , vastar , vintage , tejones , and swift i also see gas scheduled in pops at meter 3404 and 3405 . please advice . we need to resolve this as soon as possible so settlement can send out payments . thanks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_complete[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: duns number changes fyi - - - - - - - - - - - - - - - - - - - - - - forwarded by gary l payne / hou / ect on 12 / 14 / 99 02 : 35 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - from : antoine v pierre 12 / 14 / 99 02 : 34 pm to : tommy j yanowski / hou / ect @ ect , kathryn bussell / hou / ect @ ect , gary l payne / hou / ect @ ect , diane e niestrath / hou / ect @ ect , romeo d ' souza / hou / ect @ ect , michael eiben / hou / ect @ ect , clem cernosek / hou / ect @ ect , scotty gilbert / hou / ect @ ect , dave nommensen / hou / ect @ ect , david rohan / hou / ect @ ect , kevin heal / cal / ect @ ect , richard pinion / hou / ect @ ect cc : mary g gosnell / hou / ect @ ect , jason moore / hou / ect @ ect , samuel schott / hou / ect @ ect , bernice rodriguez / hou / ect @ ect subject : duns number changes i will be making these changes at 11 : 00 am on wednesday december 15 . if you do not agree or have a problem with the dnb number change please notify me , otherwise i will make the change as scheduled . dunns number change : counterparty cp id number from to cinergy resources inc . 62163 869279893 928976257 energy dynamics management , inc . 69545 825854664 088889774 south jersey resources group llc 52109 789118270 036474336 transalta energy marketing ( us ) inc . 62413 252050406 255326837 philadelphia gas works 33282 148415904 146907159 thanks , rennie 3 - 7578\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_complete[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if you see some of the documents (mails) above. These are prior to cleaning the document.\n",
    "we shall crate a function to clean the text with different sub-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = \" \".join([i.replace('*', '') for i in doc.lower().split()])\n",
    "    doc = \" \".join([i.replace(':', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('.', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('=', '') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('/', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace(')', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('(', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('\"', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('-', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i.replace('_', ' ') for i in doc.split()])\n",
    "    doc = \" \".join([i for i in doc.split() if not i.isdigit()])\n",
    "    doc = \" \".join([i for i in doc.split() if i.isalpha()])\n",
    "    doc = \" \".join([i for i in doc.split() if i not in stop])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_clear = [clean(doc) for doc in spam_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject duns number changes fyi forwarded gary l payne hou ect pm antoine v pierre pm tommy j yanowski hou ect ect kathryn bussell hou ect ect gary l payne hou ect ect diane e niestrath hou ect ect romeo souza hou ect ect michael eiben hou ect ect clem cernosek hou ect ect scotty gilbert hou ect ect dave nommensen hou ect ect david rohan hou ect ect kevin heal cal ect ect richard pinion hou ect ect cc mary g gosnell hou ect ect jason moore hou ect ect samuel schott hou ect ect bernice rodriguez hou ect ect subject duns number changes making changes wednesday december agree problem dnb number change please notify otherwise make change scheduled dunns number change counterparty cp id number cinergy resources inc energy dynamics management inc south jersey resources group llc transalta energy marketing us inc philadelphia gas works thanks rennie'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_clear[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This clean text needs to be mapped back to the data frame:\n",
    "hence we are creating a new column and assigning the text frame back to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_data[\"clean_text\"] = spam_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: christmas tree farm pictures</td>\n",
       "      <td>0</td>\n",
       "      <td>subject christmas tree farm pictures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: vastar resources , inc . gary , produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject vastar resources inc gary production h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: calpine daily gas nomination - calpin...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject calpine daily gas nomination calpine d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: re : issue fyi - see note below - alr...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject issue fyi see note already done stella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: meter 7268 nov allocation fyi . - - -...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject meter nov allocation fyi forwarded lau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0             Subject: christmas tree farm pictures       0   \n",
       "1  Subject: vastar resources , inc . gary , produ...      0   \n",
       "2  Subject: calpine daily gas nomination - calpin...      0   \n",
       "3  Subject: re : issue fyi - see note below - alr...      0   \n",
       "4  Subject: meter 7268 nov allocation fyi . - - -...      0   \n",
       "\n",
       "                                          clean_text  \n",
       "0               subject christmas tree farm pictures  \n",
       "1  subject vastar resources inc gary production h...  \n",
       "2  subject calpine daily gas nomination calpine d...  \n",
       "3  subject issue fyi see note already done stella...  \n",
       "4  subject meter nov allocation fyi forwarded lau...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_data_ABT=spam_data[[\"Class\",\"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>subject christmas tree farm pictures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>subject vastar resources inc gary production h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>subject calpine daily gas nomination calpine d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>subject issue fyi see note already done stella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>subject meter nov allocation fyi forwarded lau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                         clean_text\n",
       "0      0               subject christmas tree farm pictures\n",
       "1      0  subject vastar resources inc gary production h...\n",
       "2      0  subject calpine daily gas nomination calpine d...\n",
       "3      0  subject issue fyi see note already done stella...\n",
       "4      0  subject meter nov allocation fyi forwarded lau..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data_ABT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready for the modelling. We are going to use algorithms from sklearn package:\n",
    "\n",
    "#### Background of train/test split\n",
    "\n",
    "* Train/test split is for model evaluation\n",
    "* Model evaluation is to simulate the future\n",
    "* Past data is exchangeable for future data\n",
    "* We pretend some of our past data is coming into our future data\n",
    "* By training, predicting and evaluating the data, we can check the performance of our model\n",
    "\n",
    "#### Vectorize then split\n",
    "\n",
    "* If we vectorize then we train/test split, our document-term matrix would contain every single feature (word) in the test and training sets\n",
    "* What we want is to simulate the real world\n",
    "* We would always see words we have not seen before so this method is not realistic and we cannot properly evaluate our models\n",
    "\n",
    "#### Split then vectorize (correct way)\n",
    "\n",
    "* We do the train/test split before the CountVectorizer to properly simulate the real world where our future data contains words * we have not seen before\n",
    "* After you train your data and chose the best model, you would then train on all of your data before predicting actual future data to maximize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5172,)\n",
      "(5172,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the mail spam data) for use with COUNTVECTORIZER\n",
    "X = spam_data_ABT.clean_text\n",
    "y = spam_data_ABT.Class\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3879,)\n",
      "(1293,)\n",
      "(3879,)\n",
      "(1293,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "# by default, it splits 75% training and 25% test\n",
    "# random_state=1 for reproducibility\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. instantiate the vectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "\n",
    "# 3. fit\n",
    "vect.fit(X_train)\n",
    "\n",
    "# 4. transform training data\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# equivalently: combine fit and transform into a single step\n",
    "# this is faster and what most people would do\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3879x39159 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 242859 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1293x39159 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 70412 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "# you can see that the number of columns, 7456, is the same as what we have learned above in X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. instantiate a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. train the model \n",
    "# using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775715390564579"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    921\n",
      "1    372\n",
      "Name: Class, dtype: int64\n",
      "Null accuracy: 0    0.712297\n",
      "Name: Class, dtype: float64\n",
      "Manual null accuracy: 0.8671931083991385\n"
     ]
    }
   ],
   "source": [
    "# examine class distribution\n",
    "print(y_test.value_counts())\n",
    "# there is a majority class of 0 here, hence the classes are skewed\n",
    "\n",
    "# calculate null accuracy (for multi-class classification problems)\n",
    "# .head(1) assesses the value 1208\n",
    "null_accuracy = y_test.value_counts().head(1) / len(y_test)\n",
    "print('Null accuracy:', null_accuracy)\n",
    "\n",
    "# Manual calculation of null accuracy by always predicting the majority class\n",
    "print('Manual null accuracy:',(1208 / (1208 + 185)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[911,  10],\n",
       "       [ 19, 353]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix \n",
    "\n",
    "\n",
    "#### [True Negative, False Positive\n",
    "\n",
    "#### False Negative, True Positive]\n",
    "\n",
    "For further reading : https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3597    subject important video announcement important...\n",
       "3539    subject dear wish find mission kosovo find new...\n",
       "2456    subject hunter singing christmas program video...\n",
       "126     subject fw quips remember amateurs built ark p...\n",
       "2068    subject skydive spaceland specials skydive spa...\n",
       "301     subject expatriate zone issue expatriate zone ...\n",
       "2772    subject boat believe boat ft long boat cover b...\n",
       "2479    subject weekly fan fares delta fan fares febru...\n",
       "304     subject http www pge texas com www gtt nsf htm...\n",
       "107     subject new email yo bro new email address sav...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false positives (ham incorrectly classified as spam)\n",
    "\n",
    "X_test[y_pred_class > y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3835                 subject multiple ways get home loans\n",
       "3789                                              subject\n",
       "4932    subject hello mail transaction failed partial ...\n",
       "3743                              subject holiday e cards\n",
       "4728                                              subject\n",
       "4423                                        subject enjoy\n",
       "5168          subject str rndlen extra time word bodyhtml\n",
       "4646    subject investigation tue jan sir trouble gett...\n",
       "4657                               subject hi try nothing\n",
       "3971    subject thu may first gove rnment mo rtgage pr...\n",
       "3830           subject document please read attached file\n",
       "3815    subject feeling slze johnson rtxyj nxfgr ktrqr...\n",
       "4181    subject network wed aug kttihkefi iewmeh ckust...\n",
       "4684                                              subject\n",
       "4830                                         subject note\n",
       "4498    subject promised help people please refer ques...\n",
       "3748                    subject holiday e cards gbhzivjwl\n",
       "4593                                              subject\n",
       "4254                                              subject\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false negatives (spam incorrectly classified as ham)\n",
    "\n",
    "X_test[y_pred_class < y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under the curve(AUC) gives idea about the model efficiency:\n",
    "we can use this to compare to different models to benchmark their performance\n",
    "\n",
    "Further information: https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On similar process let's try another model and see if the accuracy changes or there is any change to the AUC\n",
    "\n",
    "For experiment we have considered logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2. instantiate a logistic regression model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 311 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791183294663574"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966405146346304"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

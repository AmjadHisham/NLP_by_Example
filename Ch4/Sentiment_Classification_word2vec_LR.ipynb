{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "# load Google's word2vec model\n",
    "\n",
    "path_to_model = '../../jupyters/vocab_coverage/word2vec_web/GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = Word2Vec.load_word2vec_format(path_to_model, binary=True)\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get word2vec's vocab\n",
    "word2vec_vocab = w2v_model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_tag(datasets=None):\n",
    "    ''' Imports the datasets into one of two dictionaries\n",
    "\n",
    "        Dicts:\n",
    "            train & test\n",
    "\n",
    "        Keys:\n",
    "            values >= 4  are \"Positive\" in both Dictionaries\n",
    "\n",
    "        '''\n",
    "    if datasets is not None:\n",
    "        train = {}\n",
    "        test = {}\n",
    "        for k, v in datasets.items(): #gives a list of tuples to iterate\n",
    "            with open(k) as fpath:\n",
    "                data = fpath.readlines()\n",
    "            for val, each_line in enumerate(data):\n",
    "                if v.endswith(\"NEG\") and v.startswith(\"TRAIN\"):\n",
    "                    train[val] = each_line\n",
    "                elif v.endswith(\"POS\") and v.startswith(\"TRAIN\"):\n",
    "                    train[val + 12500] = each_line\n",
    "                elif v.endswith(\"NEG\") and v.startswith(\"TEST\"):\n",
    "                    test[val] = each_line\n",
    "                else:\n",
    "                    test[val + 12500] = each_line\n",
    "        return train, test\n",
    "    else:\n",
    "        print('Data not found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/airwoot/Documents/Anuj/Coding/GitHub/NLP_by_Example/Ch9'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd ./../../jupyters/Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_locations = {'./../../jupyters/Data/test-neg.txt': 'TEST_NEG',\n",
    "                  './../../jupyters/Data/test-pos.txt': 'TEST_POS',\n",
    "                  './../../jupyters/Data/train-neg.txt': 'TRAIN_NEG',\n",
    "                  './../../jupyters/Data/train-pos.txt': 'TRAIN_POS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = import_tag(data_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of it s singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level it s better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [document.lower().replace('\\n', '').split() for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Tokenising...\n"
     ]
    }
   ],
   "source": [
    "# combine training and test reviews (50K) and tokenize each one of them\n",
    "\n",
    "print('Loading Data...')\n",
    "combined = train.values() + test.values()\n",
    "print('Tokenising...')\n",
    "combined = tokenizer(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> 50000\n",
      "['story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a', 'pig', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy', 'a', 'formal', 'orchestra', 'audience', 'is', 'turned', 'into', 'an', 'insane', 'violent', 'mob', 'by', 'the', 'crazy', 'chantings', 'of', 'it', 's', 'singers', 'unfortunately', 'it', 'stays', 'absurd', 'the', 'whole', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'making', 'it', 'just', 'too', 'off', 'putting', 'even', 'those', 'from', 'the', 'era', 'should', 'be', 'turned', 'off', 'the', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'to', 'a', 'third', 'grader', 'on', 'a', 'technical', 'level', 'it', 's', 'better', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'stars', 'sally', 'kirkland', 'and', 'frederic', 'forrest', 'can', 'be', 'seen', 'briefly']\n"
     ]
    }
   ],
   "source": [
    "print type(combined), len(combined)\n",
    "print combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  get the vocab size of w2v_model\n",
    "len(w2v_model.vocab.keys())\n",
    "\n",
    "# this is one way to create vocab set from our dataset \n",
    "flat_list = chain(*combined)\n",
    "dataset_vocab = set(flat_list)\n",
    "len(dataset_vocab)\n",
    "dataset_vocab_lower = [key.lower() for key in dataset_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This does 2 things - \n",
    "# 1) create a small dictionary of word to vectors for words which are only present in our dataset\n",
    "# 2) Counts the number of words in our corpus not present in Google's word to vec\n",
    "\n",
    "DIMENSION = 300\n",
    "zero_vector = np.zeros(DIMENSION)\n",
    "\n",
    "our_dict = {}\n",
    "missing_count = 0\n",
    "\n",
    "for key in dataset_vocab_lower:\n",
    "    if key in w2v_model:\n",
    "        our_dict[key] = w2v_model[key]\n",
    "    else:\n",
    "        missing_count += 1\n",
    "        #print(\"missing key %s\" %(key)) \n",
    "        our_dict[key] = zero_vector\n",
    "\n",
    "print len(dataset_vocab_lower)\n",
    "print missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordVecs(words):\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            vecs.append(model[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('food_words.txt', 'r') as infile:\n",
    "    food_words = infile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
